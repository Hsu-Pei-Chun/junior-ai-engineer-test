import openai
import json
import os
from dotenv import load_dotenv

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

def load_prompt():
    with open("quick_replies_prompt.txt", "r", encoding="utf-8") as f:
        return f.read()

# ç”Ÿæˆ Quick Replies çš„å‡½å¼
def generate_quick_replies(chat_history, faq_list, product_list):
    prompt = load_prompt()

    try:
        client = openai.OpenAI(api_key=OPENAI_API_KEY)

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": f"é€™æ˜¯éå»çš„å°è©±æ­·å²: {json.dumps(chat_history, ensure_ascii=False)}"},
                {"role": "user", "content": f"FAQ æ¸…å–®: {json.dumps(faq_list, ensure_ascii=False)}"},
                {"role": "user", "content": f"ç”¢å“æ¸…å–®: {json.dumps(product_list, ensure_ascii=False)}"},
            ],
            max_tokens=100
        )

        quick_replies = response.choices[0].message.content
        return quick_replies

    except Exception as e:
        print(f"ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

# æ¸¬è©¦å‡½å¼
if __name__ == "__main__":
    test_data = {
        "chat_history": [
            {"role": "user", "content": "æˆ‘è¦è²·é€çµ¦æ„›å¥½ 3C ç”¢å“çš„æœ‹å‹çš„ç¦®ç‰©ï¼Œè©²é¸å“ªå€‹å¥½ï¼Ÿ"},
            {"role": "assistant", "content": "æ¨è–¦é€™å¹¾æ¬¾ç†±é–€ 3C ç”¢å“ï¼š1. MacBook Pro ğŸ’» 2. iPad 0 å…ƒ ğŸ“±"}
        ],
        "faq_list": ["å¦‚ä½•æˆç‚ºæœƒå“¡", "å¦‚ä½•å–å¾—å„ªæƒ åˆ¸", "é€€è²¨æ”¿ç­–æ˜¯ä»€éº¼"],
        "product_list": ["MacBook Pro", "iPad 0 å…ƒ", "é«˜æ•ˆä¿æ¿•é¢è†œ", "ç„¡ç—•å…§è¡£"]
    }

    replies = generate_quick_replies(
        test_data["chat_history"], test_data["faq_list"], test_data["product_list"]
    )

    print("ç”¢ç”Ÿçš„ Quick Replies:")
    print(replies)
